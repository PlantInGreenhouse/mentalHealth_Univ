import requests
from bs4 import BeautifulSoup
import json
import os
import time

BASE_URL = "https://khmc.or.kr"
LIST_URL = "https://khmc.or.kr/kr/health-knowledge-medical-common/list.do?pageNo={}"
OUTPUT_FILE = "khmc_links.json"

def get_links_from_page(page_no):
    """í˜ì´ì§€ì—ì„œ ì§ˆí™˜ ë§í¬ë¥¼ ì¶”ì¶œ"""
    url = LIST_URL.format(page_no)
    response = requests.get(url)

    if response.status_code != 200:
        print(f"í˜ì´ì§€ {page_no} ì ‘ê·¼ ì‹¤íŒ¨. ìƒíƒœ ì½”ë“œ: {response.status_code}")
        return []

    soup = BeautifulSoup(response.content, "html.parser")
    links = []

    # 'tbody#list' ë‚´ì˜ <a> íƒœê·¸ì—ì„œ ë§í¬ ì¶”ì¶œ
    rows = soup.select("tbody#list a")
    for link in rows:
        relative_url = link.get("href")
        if relative_url and "view.do" in relative_url:
            full_url = BASE_URL + relative_url
            links.append(full_url)

    return links

def save_links(links, output_file):
    """ë§í¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, "w", encoding="utf-8") as file:
        json.dump(links, file, ensure_ascii=False, indent=4)
    print(f"âœ… ë§í¬ ì €ì¥ ì™„ë£Œ: {output_file}")

def main():
    all_links = []

    for page_no in range(1, 4):
        print(f"ğŸ“¦ í˜ì´ì§€ {page_no}ì—ì„œ ë§í¬ ì¶”ì¶œ ì¤‘...")
        page_links = get_links_from_page(page_no)
        all_links.extend(page_links)
        time.sleep(1)  # ì„œë²„ì— ë¶€ë‹´ì„ ì£¼ì§€ ì•Šê¸° ìœ„í•´ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€

    save_links(all_links, OUTPUT_FILE)

if __name__ == "__main__":
    main()